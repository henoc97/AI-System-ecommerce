{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparer la Base de Connaissances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_data = [\n",
    "    {\"question\": \"Comment suivre ma commande ?\", \"answer\": \"Vous pouvez suivre votre commande via votre espace client.\"},\n",
    "    {\"question\": \"Quels sont les modes de paiement disponibles ?\", \"answer\": \"Nous acceptons les paiements par carte, PayPal et Apple Pay.\"},\n",
    "]\n",
    "\n",
    "product_data = [\n",
    "    {\"id\": 1, \"name\": \"Laptop\", \"description\": \"Un ordinateur portable performant avec 16 Go de RAM.\"},\n",
    "    {\"id\": 2, \"name\": \"Smartphone\", \"description\": \"Un téléphone avec un écran AMOLED et une excellente caméra.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexer la Base de Connaissances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382a8d26511640a392f1ff89e6b4f56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68131f00c614db3bc4d01b3d34a4fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe26b4ae6664b3584af7aed0853109a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4606a37aa66a473984a848a3391199de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3198767bb444839d1c62a7f3a66fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77927ff498254d61854a9bcbf14c474d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc034e05b934fb08c992e55891c35a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84531e4641904f25b5f21c87bfaddb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c34040595425097a6509934ef5723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067619c287ac46ed9e9cdc0cb9730a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68ec6e85fb94d8cb9a122fde0c08290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Charger un modèle d'encodage\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Indexer les données FAQ\n",
    "faq_df = pd.DataFrame(faq_data)\n",
    "faq_df['embedding'] = faq_df['question'].apply(lambda x: encoder.encode(x))\n",
    "\n",
    "# Indexer le catalogue produits\n",
    "product_df = pd.DataFrame(product_data)\n",
    "product_df['embedding'] = product_df['description'].apply(lambda x: encoder.encode(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechercher dans la Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : J'ai payé mon produit, c'est quoi la suite ?\n",
      "Réponse : Vous pouvez suivre votre commande via votre espace client.\n",
      "Question : J'ai besoin d'un pc de développement logiciel ?\n",
      "Réponse : Un ordinateur portable performant avec 16 Go de RAM.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(query, embeddings, data, top_k=1):\n",
    "    query_embedding = encoder.encode(query).reshape(1, -1)\n",
    "    similarities = cosine_similarity(query_embedding, np.vstack(embeddings))\n",
    "    best_indices = similarities[0].argsort()[-top_k:][::-1]\n",
    "    return [data.iloc[idx] for idx in best_indices]\n",
    "\n",
    "# Exemple : Recherche dans la FAQ\n",
    "query = \"J'ai payé mon produit, c'est quoi la suite ?\"\n",
    "faq_result = find_most_similar(query, faq_df['embedding'], faq_df)\n",
    "print(f'Question : {query}')\n",
    "print(f\"Réponse : {faq_result[0]['answer']}\")\n",
    "\n",
    "# Exemple : Recherche dans la desc des produits\n",
    "query = \"J'ai besoin d'un pc de développement logiciel ?\"\n",
    "prod_result = find_most_similar(query, product_df['embedding'], product_df)\n",
    "print(f'Question : {query}')\n",
    "print(f\"Réponse : {prod_result[0]['description']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrage Contextuel pour le Modèle Génératif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context(query, product_df, top_k=3):\n",
    "    \"\"\"\n",
    "    Filtre les descriptions de produits pertinentes en fonction de la question.\n",
    "    \"\"\"\n",
    "    product_results = find_most_similar(query, product_df['embedding'], product_df, top_k=top_k)\n",
    "    return \" \".join([res['description'] for res in product_results])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intégrer un Modèle Génératif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée : Un ordinateur portable performant avec 16 Go de RAM.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger un modèle pré-entraîné\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Exemple : Génération de réponse\n",
    "question = \"Bonjour je cherche un ordinateur ?\"\n",
    "context = filter_context(query, product_df, top_k=3)\n",
    "response = qa_pipeline(question=question, context=context)\n",
    "print(f\"Réponse générée : {response['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis désolé, je ne peux pas répondre à cette question pour le moment.\n",
      "FAQ : Vous pouvez suivre votre commande via votre espace client.\n"
     ]
    }
   ],
   "source": [
    "def chatbot(query):\n",
    "    # Étape 1 : Rechercher dans la FAQ\n",
    "    faq_result = find_most_similar(query, faq_df['embedding'], faq_df, top_k=1)\n",
    "    if faq_result and cosine_similarity(encoder.encode(query).reshape(1, -1), [faq_result[0]['embedding']])[0][0] > 0.75:\n",
    "        return f\"FAQ : {faq_result[0]['answer']}\"\n",
    "    \n",
    "    # Étape 2 : Générer une réponse pour les requêtes complexes\n",
    "    context = filter_context(query, product_df, top_k=3)\n",
    "    response = qa_pipeline(question=query, context=context)\n",
    "    if response['score'] > 0.7:\n",
    "        return f\"Generated: {response['answer']}\"\n",
    "    \n",
    "    return \"Je suis désolé, je ne peux pas répondre à cette question pour le moment.\"\n",
    "\n",
    "# Tester le chatbot\n",
    "print(chatbot(\"Quels produits ont 16 Go de RAM ?\"))\n",
    "print(chatbot(\"Comment suivre ma commande ?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_context(user_id):\n",
    "    # Exemple : récupérer des commandes ou des interactions\n",
    "    orders = [\n",
    "        {\"id\": 1, \"product\": \"Laptop\", \"status\": \"En cours de livraison\"},\n",
    "        {\"id\": 2, \"product\": \"Smartphone\", \"status\": \"Livré\"},\n",
    "    ]\n",
    "    return \" \".join([f\"Commande {order['id']} : {order['product']} ({order['status']})\" for order in orders])\n",
    "\n",
    "# Mise à jour du chatbot\n",
    "def chatbot(query, user_id=None):\n",
    "    # Étape 1 : Rechercher dans la FAQ\n",
    "    faq_result = find_most_similar(query, faq_df['embedding'], faq_df, top_k=1)\n",
    "    if faq_result and cosine_similarity(encoder.encode(query).reshape(1, -1), [faq_result[0]['embedding']])[0][0] > 0.75:\n",
    "        return f\"FAQ : {faq_result[0]['answer']}\"\n",
    "    \n",
    "    # Étape 2 : Générer une réponse pour les requêtes complexes\n",
    "    context = filter_context(query, product_df, top_k=3)\n",
    "    if user_id:\n",
    "        user_context = get_user_context(user_id)\n",
    "        context = f\"{context} {user_context}\"\n",
    "    \n",
    "    response = qa_pipeline(question=query, context=context)\n",
    "    if response['score'] > 0.7:\n",
    "        return f\"Generated: {response['answer']}\"\n",
    "    \n",
    "    return \"Je suis désolé, je ne peux pas répondre à cette question pour le moment.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je suis désolé, je ne peux pas répondre à cette question pour le moment.\n",
      "FAQ : Vous pouvez suivre votre commande via votre espace client.\n"
     ]
    }
   ],
   "source": [
    "# Test avec utilisateur\n",
    "print(chatbot(\"Quels produits ont 16 Go de RAM ?\", user_id=1))\n",
    "print(chatbot(\"Comment suivre ma commande ?\", user_id=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
